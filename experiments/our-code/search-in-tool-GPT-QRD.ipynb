{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ahmedalsayed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util, models\n",
    "from sklearn.feature_extraction import _stop_words as stop_words\n",
    "from tqdm.notebook import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "import torch\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_enc_weights = '../models/bienc-exp7/'\n",
    "cr_enc_weights = '../models/crenc-readme-exp4/'\n",
    "data_folder = 'generated5'\n",
    "top_k = 50\n",
    "use_base = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ahmedalsayed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ahmedalsayed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ahmedalsayed/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question Link</th>\n",
       "      <th>Question Title</th>\n",
       "      <th>Question Body</th>\n",
       "      <th>Accepted Answer Body</th>\n",
       "      <th>link</th>\n",
       "      <th>readme</th>\n",
       "      <th>docker</th>\n",
       "      <th>readme_short</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>390150</td>\n",
       "      <td>Authenticating against Active Directory with J...</td>\n",
       "      <td>&lt;p&gt;I have a simple task of authenticating agai...</td>\n",
       "      <td>&lt;p&gt;Here's the code I put together based on exa...</td>\n",
       "      <td>https://github.com/jenkinsci/active-directory-...</td>\n",
       "      <td>Active Directory plugin for Jenkins\\n=========...</td>\n",
       "      <td>Dockerfile from src/test/resources/fixture/Doc...</td>\n",
       "      <td>Active Directory plugin for Jenkins ==========...</td>\n",
       "      <td>active directory plugin jenkins build status j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1197678</td>\n",
       "      <td>Using Thrift with Delphi Win32</td>\n",
       "      <td>&lt;p&gt;I'm interested in connecting to the Evernot...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;Old Answer Replaced thanks to Leo:&lt;...</td>\n",
       "      <td>https://github.com/apache/thrift</td>\n",
       "      <td>Apache Thrift\\n=============\\nIntroduction\\n==...</td>\n",
       "      <td>Dockerfile from build/docker/msvc2017/Dockerfi...</td>\n",
       "      <td>Apache Thrift ============= Introduction =====...</td>\n",
       "      <td>apache thrift introduction thrift lightweight ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Question Link                                     Question Title  \\\n",
       "0         390150  Authenticating against Active Directory with J...   \n",
       "1        1197678                     Using Thrift with Delphi Win32   \n",
       "\n",
       "                                       Question Body  \\\n",
       "0  <p>I have a simple task of authenticating agai...   \n",
       "1  <p>I'm interested in connecting to the Evernot...   \n",
       "\n",
       "                                Accepted Answer Body  \\\n",
       "0  <p>Here's the code I put together based on exa...   \n",
       "1  <p><strong>Old Answer Replaced thanks to Leo:<...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://github.com/jenkinsci/active-directory-...   \n",
       "1                   https://github.com/apache/thrift   \n",
       "\n",
       "                                              readme  \\\n",
       "0  Active Directory plugin for Jenkins\\n=========...   \n",
       "1  Apache Thrift\\n=============\\nIntroduction\\n==...   \n",
       "\n",
       "                                              docker  \\\n",
       "0  Dockerfile from src/test/resources/fixture/Doc...   \n",
       "1  Dockerfile from build/docker/msvc2017/Dockerfi...   \n",
       "\n",
       "                                        readme_short  \\\n",
       "0  Active Directory plugin for Jenkins ==========...   \n",
       "1  Apache Thrift ============= Introduction =====...   \n",
       "\n",
       "                                              answer  \n",
       "0  active directory plugin jenkins build status j...  \n",
       "1  apache thrift introduction thrift lightweight ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Define stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def advanced_clean(text):\n",
    "    # Basic cleaning (e.g., removing special characters, non-alphanumeric characters)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Reconstruct the text\n",
    "    return ' '.join(tokens)\n",
    "file_path = '../data/20231004_data.xlsx' \n",
    "df = pd.read_excel(file_path, index_col=0)\n",
    "\n",
    "df['readme_short'] = df['readme_short'].astype(str)\n",
    "df['docker'] = df['docker'].astype(str)\n",
    "df['answer'] = df.apply(lambda row: advanced_clean(' '.join(row['readme_short'].split()[:400] + row['docker'].split()[:200])), axis=1)\n",
    "\n",
    "\n",
    "df.head(2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def bm25_tokenizer(text):\n",
    "  tokenized_doc = []\n",
    "  for token in text.lower().split():\n",
    "    token = token.strip(string.punctuation)\n",
    "\n",
    "    if len(token) > 0 and token not in english_stopwords:\n",
    "      tokenized_doc.append(token)\n",
    "      \n",
    "  return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_base:\n",
    "    word_embedding_model = models.Transformer('distilroberta-base', max_seq_length=350)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "    bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    cr_encoder = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-6')\n",
    "else:\n",
    "    bi_encoder = SentenceTransformer(bi_enc_weights)\n",
    "    cr_encoder = CrossEncoder(cr_enc_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/Documents/final-code-microservice-paper/experiments/test_passage_100.json', 'r') as f:\n",
    "    val_passage = json.load(f)\n",
    "\n",
    "with open('/Users/Documents/final-code-microservice-paper/experiments/test_corpus_100.json', 'r') as f:  # Note the _100 in the filename\n",
    "    val_corpus = json.load(f)\n",
    "\n",
    "val_query_answer = {}\n",
    "val_query_readme = {}\n",
    "\n",
    "for idx, rel in val_passage.items():\n",
    "    if idx not in val_corpus:  \n",
    "        continue  \n",
    "\n",
    "    pos = rel[0]  \n",
    "\n",
    "   \n",
    "    try:\n",
    "        readme = df.loc[pos, 'answer']\n",
    "    except (KeyError, IndexError):\n",
    "       \n",
    "        continue\n",
    "\n",
    "    questions = []\n",
    "    for p in rel:  \n",
    "       \n",
    "        try:\n",
    "            question = df.loc[p, 'Question Title']\n",
    "            questions.append(question)\n",
    "        except (KeyError, IndexError):\n",
    "            \n",
    "            pass\n",
    "\n",
    "   \n",
    "    if questions:\n",
    "        val_query_answer[idx] = questions\n",
    "        val_query_readme[idx] = [readme]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten(text):\n",
    "    tmp = text.split()[:512]\n",
    "    return ' '.join(tmp)\n",
    " \n",
    "\n",
    "val_text = list(val_corpus.values())\n",
    "\n",
    "val_readme = []\n",
    "for t in val_text:\n",
    "    val_readme.append(df.loc[df['Question Title'] == t, 'answer'].values[0])\n",
    "\n",
    "\n",
    "with open(\"/Users/Documents/final-code-microservice-paper/experiments/data/embeddings_GPT.json\", \"r\") as jsonfile:\n",
    "    embeddings_dict = json.load(jsonfile)\n",
    "\n",
    "\n",
    "val_emb_tensors = []\n",
    "\n",
    "\n",
    "\n",
    "title_to_id = dict(zip(df['Question Title'], df.index))\n",
    "\n",
    "\n",
    "for text in val_text:\n",
    "    \n",
    "    text_id = title_to_id[text]\n",
    "    embedding = embeddings_dict.get(str(text_id))  \n",
    "    if embedding:  \n",
    "        val_emb_tensors.append(torch.tensor(embedding))\n",
    "\n",
    "\n",
    "val_emb = torch.stack(val_emb_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 248625.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tokenized_corpus = []\n",
    "for idx, passage in tqdm(val_corpus.items()):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:43<00:00,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Questions Answered Correctly: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Total Questions Answered Correctly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, ndcg_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"No positive class found in y_true, recall is set to one for all thresholds.\")\n",
    "\n",
    "def compute_cosine_similarity(query_embedding, corpus_embeddings):\n",
    "    # Normalize embeddings\n",
    "    if len(query_embedding.shape) == 1:\n",
    "        query_embedding = query_embedding / torch.norm(query_embedding, keepdim=True)\n",
    "        query_embedding = query_embedding.unsqueeze(0)  # Add an additional dimension\n",
    "    else:\n",
    "        query_embedding = query_embedding / torch.norm(query_embedding, dim=1, keepdim=True)\n",
    "    \n",
    "    corpus_embeddings = corpus_embeddings / torch.norm(corpus_embeddings, dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_similarities = torch.mm(query_embedding, corpus_embeddings.transpose(0, 1))\n",
    "    \n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def forward_pass_rerank(query, precomputed_embedding=None, val_embeddings=None, top_k=50):\n",
    "    \n",
    "    if precomputed_embedding is None:\n",
    "        q_emb = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    else:\n",
    "        q_emb = precomputed_embedding\n",
    "\n",
    "    # Ensure the query embedding is 2-dimensional\n",
    "    if len(q_emb.shape) == 1:\n",
    "        q_emb = q_emb.unsqueeze(0)\n",
    "\n",
    "    if val_embeddings is None:\n",
    "        raise ValueError(\"No embeddings provided for the validation set.\")\n",
    "    \n",
    "    \n",
    "    if val_embeddings.shape[1] != q_emb.shape[1]:\n",
    "        val_embeddings = val_embeddings.transpose(0, 1)\n",
    "    \n",
    "    cosine_similarities = compute_cosine_similarity(q_emb, val_embeddings)\n",
    "    \n",
    "\n",
    "    top_indices = torch.topk(cosine_similarities, k=top_k+1, dim=1).indices[0].tolist()\n",
    "    hits = [{'corpus_id': index, 'score': cosine_similarities[0, index].item()} for index in top_indices]\n",
    "   \n",
    "\n",
    "    cross_inputs = []\n",
    "    to_remove = -1\n",
    "    for hit in hits:\n",
    "        readme = val_readme[hit['corpus_id']]\n",
    "        text = val_text[hit['corpus_id']]\n",
    "        if query == text:\n",
    "            to_remove = hits.index(hit)\n",
    "        cross_inputs.append([query, readme])\n",
    "    cross_scores = cr_encoder.predict(cross_inputs)\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross_score'] = cross_scores[idx]\n",
    "    if to_remove != -1: del hits[to_remove]\n",
    "    hits = hits[:top_k]\n",
    "\n",
    "    return hits\n",
    "\n",
    "\n",
    "total_questions_answered_correctly = 0\n",
    "\n",
    "for (query_key, answers), (_, readme) in tqdm(zip(val_query_answer.items(), val_query_readme.items()), total=len(val_query_answer)):\n",
    "    query= val_corpus[query_key]\n",
    "    # Fetch precomputed embedding\n",
    "    query_id = title_to_id[query]\n",
    "    precomputed_query_embedding = torch.tensor(embeddings_dict.get(str(query_id)))\n",
    "    hits = forward_pass_rerank(query, precomputed_embedding=precomputed_query_embedding, val_embeddings=val_emb, top_k=top_k)\n",
    "\n",
    "    r = val_query_readme[query_key]\n",
    "    \n",
    "    hit_info_list = []\n",
    "    \n",
    "    for hit in hits[:5]:\n",
    "        readme = val_readme[hit['corpus_id']]\n",
    "        cscore_output_cr = cr_encoder.predict([query, readme])  \n",
    "        cscore_cr = cscore_output_cr[0] if isinstance(cscore_output_cr, (list, tuple, np.ndarray)) else cscore_output_cr\n",
    "        hit_info = {\n",
    "            'h_text': readme,\n",
    "            'cscore': cscore_cr,\n",
    "            'is_correct': \"Yes\" if readme in r else \"No\"\n",
    "        }\n",
    "        hit_info_list.append(hit_info)\n",
    "    \n",
    "    question_answered_correctly = any(hit_info['is_correct'] == \"Yes\" for hit_info in hit_info_list)\n",
    "    if question_answered_correctly:\n",
    "        total_questions_answered_correctly += 1\n",
    "\n",
    "print(f\"Total Questions Answered Correctly: {total_questions_answered_correctly}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:42<00:00,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Correct Answers: 185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Total Correcte Answeres \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, ndcg_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"No positive class found in y_true, recall is set to one for all thresholds.\")\n",
    "\n",
    "def compute_cosine_similarity(query_embedding, corpus_embeddings):\n",
    "    # Normalize embeddings\n",
    "    if len(query_embedding.shape) == 1:\n",
    "        query_embedding = query_embedding / torch.norm(query_embedding, keepdim=True)\n",
    "        query_embedding = query_embedding.unsqueeze(0)  # Add an additional dimension\n",
    "    else:\n",
    "        query_embedding = query_embedding / torch.norm(query_embedding, dim=1, keepdim=True)\n",
    "    \n",
    "    corpus_embeddings = corpus_embeddings / torch.norm(corpus_embeddings, dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_similarities = torch.mm(query_embedding, corpus_embeddings.transpose(0, 1))\n",
    "    \n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "\n",
    "def forward_pass_rerank(query, precomputed_embedding=None, val_embeddings=None, top_k=50):\n",
    "    \n",
    "    if precomputed_embedding is None:\n",
    "        q_emb = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    else:\n",
    "        q_emb = precomputed_embedding\n",
    "\n",
    "    # Ensure the query embedding is 2-dimensional\n",
    "    if len(q_emb.shape) == 1:\n",
    "        q_emb = q_emb.unsqueeze(0)\n",
    "\n",
    "    if val_embeddings is None:\n",
    "        raise ValueError(\"No embeddings provided for the validation set.\")\n",
    "    \n",
    "    \n",
    "    if val_embeddings.shape[1] != q_emb.shape[1]:\n",
    "        val_embeddings = val_embeddings.transpose(0, 1)\n",
    "    \n",
    "    cosine_similarities = compute_cosine_similarity(q_emb, val_embeddings)\n",
    "    \n",
    "\n",
    "    top_indices = torch.topk(cosine_similarities, k=top_k+1, dim=1).indices[0].tolist()\n",
    "    hits = [{'corpus_id': index, 'score': cosine_similarities[0, index].item()} for index in top_indices]\n",
    "   \n",
    "\n",
    "    cross_inputs = []\n",
    "    to_remove = -1\n",
    "    for hit in hits:\n",
    "        readme = val_readme[hit['corpus_id']]\n",
    "        text = val_text[hit['corpus_id']]\n",
    "        if query == text:\n",
    "            to_remove = hits.index(hit)\n",
    "        cross_inputs.append([query, readme])\n",
    "    cross_scores = cr_encoder.predict(cross_inputs)\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross_score'] = cross_scores[idx]\n",
    "    if to_remove != -1: del hits[to_remove]\n",
    "    hits = hits[:top_k]\n",
    "\n",
    "    return hits\n",
    "\n",
    "total_correct = 0\n",
    "\n",
    "for (query_key, answers), (_, readme) in tqdm(zip(val_query_answer.items(), val_query_readme.items()), total=len(val_query_answer)):\n",
    "    query= val_corpus[query_key]\n",
    "    # Fetch precomputed embedding\n",
    "    query_id = title_to_id[query]\n",
    "    precomputed_query_embedding = torch.tensor(embeddings_dict.get(str(query_id)))\n",
    "    hits = forward_pass_rerank(query, precomputed_embedding=precomputed_query_embedding, val_embeddings=val_emb, top_k=top_k)\n",
    "\n",
    "    r = val_query_readme[query_key]\n",
    "    \n",
    "    hit_info_list = []\n",
    "    \n",
    "    for hit in hits[:5]:\n",
    "        readme = val_readme[hit['corpus_id']]\n",
    "        cscore_output_cr = cr_encoder.predict([query, readme])  \n",
    "        cscore_cr = cscore_output_cr[0] if isinstance(cscore_output_cr, (list, tuple, np.ndarray)) else cscore_output_cr\n",
    "        hit_info = {\n",
    "            'h_text': readme,\n",
    "            'cscore': cscore_cr,\n",
    "            'is_correct': \"Yes\" if readme in r else \"No\"\n",
    "        }\n",
    "        hit_info_list.append(hit_info)\n",
    "    \n",
    "    correct_count = sum(1 for hit_info in hit_info_list if hit_info['is_correct'] == \"Yes\")\n",
    "    total_correct += correct_count\n",
    "\n",
    "print(f\"Total Correct Answers: {total_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:04<07:52,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Start new K8s Pod within K8s Pod without \"Privileges\"\n",
      "\tHit: Start kubernetes container with specific command - Correct: No\n",
      "\tHit: Kubernetes simple authentication - Correct: Yes\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\tHit: Kubernetes kube-dns pod is pending - Correct: Yes\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:09<07:38,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Service proxy with affinity based on URL\n",
      "\tHit: Access nodeport via kube-proxy from another machine - Correct: Yes\n",
      "\tHit: Dynamically resolve a pods address for get request - Correct: Yes\n",
      "\tHit: Mapping an external http load balancer to a Kubernetes Cluster - Correct: No\n",
      "\tHit: AWS EKS Guestbook url not accessible - Correct: No\n",
      "\tHit: Bind to multiple ip addresses in a single docker container - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:14<07:31,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes Storage on bare-metal/private cloud\n",
      "\tHit: Kubernetes - Best way to initialize persistent volume for a database deployment - Correct: No\n",
      "\tHit: Kubernetes Ephemeral Storage Limit and Container Logs - Correct: Yes\n",
      "\tHit: Kubernetes simple authentication - Correct: Yes\n",
      "\tHit: Running dashboard inside play-with-kubernetes - Correct: No\n",
      "\tHit: Kafka on Kubernetes multi-node - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:32<07:12,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can I get a Pod's pause container ID via the Kubernetes API?\n",
      "\tHit: Kubernetes Watch Pod Events with api - Correct: Yes\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Kubernetes kube-dns pod is pending - Correct: Yes\n",
      "\tHit: Start new K8s Pod within K8s Pod without \"Privileges\" - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:37<07:06,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Create PersistentVolumeClaim imperative way?\n",
      "\tHit: Kubernetes - Best way to initialize persistent volume for a database deployment - Correct: No\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: Setup commands for Mesos and Kubernetes on Docker? - Correct: Yes\n",
      "\tHit: Kubernetes fsGroup not changing file ownership on PersistentVolume - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:41<07:02,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Alternative to Kubernetes rolling update in rest api\n",
      "\tHit: Are Kubernetes API calls secret update and configmap update atomic calls? - Correct: No\n",
      "\tHit: Kubernetes Watch Pod Events with api - Correct: Yes\n",
      "\tHit: Restricted Kubernetes dashboard? - Correct: No\n",
      "\tHit: Running dashboard inside play-with-kubernetes - Correct: No\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:46<06:58,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: MySQL - Galera OR Kubernetes Replication Controller?\n",
      "\tHit: Kafka on Kubernetes multi-node - Correct: Yes\n",
      "\tHit: Setup commands for Mesos and Kubernetes on Docker? - Correct: Yes\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\tHit: Kubernetes - Best way to initialize persistent volume for a database deployment - Correct: No\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:55<06:49,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Authentication views for Laravel 5.1\n",
      "\tHit: Force logout of specific user by user id in Laravel - Correct: Yes\n",
      "\tHit: Best way to make restfull API in Laravel - Correct: Yes\n",
      "\tHit: Kubernetes simple authentication - Correct: No\n",
      "\tHit: Create user with LDAP authentification in airflow 2.1.4 - Correct: No\n",
      "\tHit: React Native Phone-based login - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [01:05<06:49,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes Ephemeral Storage Limit and Container Logs\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\tHit: Kubernetes CGE logging to kibana logging - Correct: Yes\n",
      "\tHit: Kubernetes - Best way to initialize persistent volume for a database deployment - Correct: No\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: kubernetes label name 63 character limit - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [01:10<06:43,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the deployment controller sync period for kube-controller-manager?\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: What's the status of pods returned by the cluster API under ReplicationControllerStatus.replicas - Correct: Yes\n",
      "\tHit: Are Kubernetes API calls secret update and configmap update atomic calls? - Correct: No\n",
      "\tHit: Kubernetes, HPA scales up with every deployment - Correct: Yes\n",
      "\tHit: Kubernetes Ephemeral Storage Limit and Container Logs - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [01:14<06:36,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Access nodeport via kube-proxy from another machine\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: Running dashboard inside play-with-kubernetes - Correct: No\n",
      "\tHit: Kubernetes kube-proxy iptables rules seem to be redundant - Correct: Yes\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [01:24<06:23,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Force logout of specific user by user id in Laravel\n",
      "\tHit: Authentication views for Laravel 5.1 - Correct: Yes\n",
      "\tHit: Best way to make restfull API in Laravel - Correct: Yes\n",
      "\tHit: Create user with LDAP authentification in airflow 2.1.4 - Correct: No\n",
      "\tHit: Spring security JWT refresh token not expiring - Correct: No\n",
      "\tHit: React Native Phone-based login - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [01:28<06:18,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: enableServiceLinks=false doesn't disable Kubernetes clusterIP default service environment variables to be injected into pod\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Kubernetes kube-proxy iptables rules seem to be redundant - Correct: Yes\n",
      "\tHit: Start new K8s Pod within K8s Pod without \"Privileges\" - Correct: Yes\n",
      "\tHit: when and where a service ip created for kubernetes static pods - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [01:47<05:58,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Configure starting index of StatefulSet's pods in Kubernetes\n",
      "\tHit: Kubernetes kube-dns pod is pending - Correct: Yes\n",
      "\tHit: when and where a service ip created for kubernetes static pods - Correct: Yes\n",
      "\tHit: Kubernetes - Best way to initialize persistent volume for a database deployment - Correct: No\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [01:52<05:53,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What's the status of pods returned by the cluster API under ReplicationControllerStatus.replicas\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Kubernetes kube-dns pod is pending - Correct: Yes\n",
      "\tHit: Can I get a Pod's pause container ID via the Kubernetes API? - Correct: Yes\n",
      "\tHit: What is the deployment controller sync period for kube-controller-manager? - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [01:56<05:49,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: K8 Pod Lifetime: Is Cleanup Necessary?\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: Kubernetes kube-dns pod is pending - Correct: Yes\n",
      "\tHit: Kubernetes Ephemeral Storage Limit and Container Logs - Correct: Yes\n",
      "\tHit: Restricted Kubernetes dashboard? - Correct: No\n",
      "\tHit: Kubernetes - Best way to initialize persistent volume for a database deployment - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [02:38<05:08,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes fsGroup not changing file ownership on PersistentVolume\n",
      "\tHit: Kubernetes - Best way to initialize persistent volume for a database deployment - Correct: No\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\tHit: Kubernetes (1.10) mountPropagation: Bidirectional not working. - Correct: Yes\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\tHit: Kubernetes Ephemeral Storage Limit and Container Logs - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [02:43<05:03,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Assigning IP address to docker containers?\n",
      "\tHit: Giving a docker container a routable ip address - Correct: Yes\n",
      "\tHit: Bind to multiple ip addresses in a single docker container - Correct: Yes\n",
      "\tHit: Setup commands for Mesos and Kubernetes on Docker? - Correct: No\n",
      "\tHit: Kubernetes: spec.clusterIP: invalid value? - Correct: No\n",
      "\tHit: Restricted Kubernetes dashboard? - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [02:52<04:53,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes with Vagrant nodes won't register\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\tHit: Airflow scheduler can not connect to Kubernetes service api - Correct: No\n",
      "\tHit: kubelet fails to start with rocket - Correct: Yes\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\tHit: Kubernetes kube-dns pod is pending - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [03:02<04:43,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes Watch Pod Events with api\n",
      "\tHit: All possible Kubernetes events with type - Correct: Yes\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: Running dashboard inside play-with-kubernetes - Correct: No\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Kubernetes kube-dns pod is pending - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [03:06<04:38,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: when and where a service ip created for kubernetes static pods\n",
      "\tHit: Kubernetes kube-dns pod is pending - Correct: Yes\n",
      "\tHit: Configure starting index of StatefulSet's pods in Kubernetes - Correct: Yes\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [03:20<04:24,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Unknown and noisy fields when executing kubectl run and outputting to yaml\n",
      "\tHit: kubelet fails to start with rocket - Correct: Yes\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: rendering env-var inside kubernetes kubeconfig yaml file - Correct: Yes\n",
      "\tHit: Running dashboard inside play-with-kubernetes - Correct: No\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [03:29<04:16,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubectl describe returns 404 for Ingresses\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\tHit: Kubernetes: spec.clusterIP: invalid value? - Correct: Yes\n",
      "\tHit: kubectl : unable to recognize \"csr.yaml\": no matches for kind \"CertificateSigningRequest\" in version \"certificates.k8s.io/v1\" - Correct: Yes\n",
      "\tHit: Kubernetes kube-dns pod is pending - Correct: Yes\n",
      "\tHit: AWS EKS Guestbook url not accessible - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [03:34<04:12,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Dynamically resolve a pods address for get request\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: when and where a service ip created for kubernetes static pods - Correct: Yes\n",
      "\tHit: Can I get a Pod's pause container ID via the Kubernetes API? - Correct: Yes\n",
      "\tHit: Access nodeport via kube-proxy from another machine - Correct: Yes\n",
      "\tHit: Giving a docker container a routable ip address - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [03:44<04:05,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: kubernetes label name 63 character limit\n",
      "\tHit: Kubernetes Ephemeral Storage Limit and Container Logs - Correct: Yes\n",
      "\tHit: What is the syntax of annotation values in Kubernetes? - Correct: Yes\n",
      "\tHit: Kubernetes: spec.clusterIP: invalid value? - Correct: Yes\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\tHit: Kubernetes simple authentication - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [03:48<03:59,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Best way to make restfull API in Laravel\n",
      "\tHit: Authentication views for Laravel 5.1 - Correct: Yes\n",
      "\tHit: Alternative to Kubernetes rolling update in rest api - Correct: No\n",
      "\tHit: Force logout of specific user by user id in Laravel - Correct: Yes\n",
      "\tHit: Project layout with vagrant, docker and git - Correct: No\n",
      "\tHit: Which way is the best for running background processes? - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [03:53<03:54,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes: spec.clusterIP: invalid value?\n",
      "\tHit: What is the syntax of annotation values in Kubernetes? - Correct: Yes\n",
      "\tHit: Kubernetes with Vagrant nodes won't register - Correct: Yes\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\tHit: kubelet fails to start with rocket - Correct: Yes\n",
      "\tHit: Restricted Kubernetes dashboard? - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [04:02<03:44,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: k8s - Significance of ReplicaSet matchLabel selector\n",
      "\tHit: kubernetes label name 63 character limit - Correct: Yes\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Kafka on Kubernetes multi-node - Correct: Yes\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: What's the status of pods returned by the cluster API under ReplicationControllerStatus.replicas - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [04:07<03:39,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: File Transform task does not work in Release pipeline for Xamarin.Forms\n",
      "\tHit: Github Release Azure Pipeline task - tagpattern wildcards does not work - Correct: Yes\n",
      "\tHit: StreamingResponseBodyReturnValueHandler does not use applicationTaskExecutor - Correct: No\n",
      "\tHit: http auth does not work with TortoiseGit - Correct: No\n",
      "\tHit: Azure DevOps & Azure CLI: Unable to locate executable file - Correct: Yes\n",
      "\tHit: Project layout with vagrant, docker and git - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [04:12<03:34,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the syntax of annotation values in Kubernetes?\n",
      "\tHit: Kubernetes: spec.clusterIP: invalid value? - Correct: Yes\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: kubernetes label name 63 character limit - Correct: Yes\n",
      "\tHit: Kubernetes simple authentication - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [04:16<03:29,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Bind to multiple ip addresses in a single docker container\n",
      "\tHit: Giving a docker container a routable ip address - Correct: Yes\n",
      "\tHit: Assigning IP address to docker containers? - Correct: Yes\n",
      "\tHit: Reuse host binaries or share between containers in Docker - Correct: No\n",
      "\tHit: Run Jupyter Notebook in the Background on Docker - Correct: No\n",
      "\tHit: Docker, Supervisord and supervisor-stdout - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [04:35<03:13,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Github Release Azure Pipeline task - tagpattern wildcards does not work\n",
      "\tHit: File Transform task does not work in Release pipeline for Xamarin.Forms - Correct: Yes\n",
      "\tHit: Azure DevOps & Azure CLI: Unable to locate executable file - Correct: Yes\n",
      "\tHit: http auth does not work with TortoiseGit - Correct: No\n",
      "\tHit: Project layout with vagrant, docker and git - Correct: No\n",
      "\tHit: Gogits do not start as docker container - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [04:49<02:57,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes simple authentication\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\tHit: Running dashboard inside play-with-kubernetes - Correct: No\n",
      "\tHit: Restricted Kubernetes dashboard? - Correct: No\n",
      "\tHit: Kafka on Kubernetes multi-node - Correct: Yes\n",
      "\tHit: Start new K8s Pod within K8s Pod without \"Privileges\" - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [05:08<02:39,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: rendering env-var inside kubernetes kubeconfig yaml file\n",
      "\tHit: Running dashboard inside play-with-kubernetes - Correct: No\n",
      "\tHit: Unknown and noisy fields when executing kubectl run and outputting to yaml - Correct: Yes\n",
      "\tHit: Kubernetes CGE logging to kibana logging - Correct: Yes\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [05:27<02:21,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: All possible Kubernetes events with type\n",
      "\tHit: Kubernetes Watch Pod Events with api - Correct: Yes\n",
      "\tHit: Kubernetes CGE logging to kibana logging - Correct: Yes\n",
      "\tHit: Kafka on Kubernetes multi-node - Correct: Yes\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [05:32<02:17,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes, HPA scales up with every deployment\n",
      "\tHit: Kafka on Kubernetes multi-node - Correct: Yes\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\tHit: Kubernetes - Best way to initialize persistent volume for a database deployment - Correct: No\n",
      "\tHit: Running dashboard inside play-with-kubernetes - Correct: No\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [05:46<02:02,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes (1.10) mountPropagation: Bidirectional not working.\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\tHit: Kubernetes fsGroup not changing file ownership on PersistentVolume - Correct: Yes\n",
      "\tHit: Kubernetes with Vagrant nodes won't register - Correct: Yes\n",
      "\tHit: Kubernetes: spec.clusterIP: invalid value? - Correct: Yes\n",
      "\tHit: Airflow scheduler can not connect to Kubernetes service api - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [05:55<01:53,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes CGE logging to kibana logging\n",
      "\tHit: Kubernetes Ephemeral Storage Limit and Container Logs - Correct: Yes\n",
      "\tHit: Kafka on Kubernetes multi-node - Correct: Yes\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\tHit: Running dashboard inside play-with-kubernetes - Correct: No\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [06:00<01:48,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes prometheus metrics for running pods and nodes?\n",
      "\tHit: Running dashboard inside play-with-kubernetes - Correct: No\n",
      "\tHit: can i use prometheus without docker and kuber? - Correct: No\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Kafka on Kubernetes multi-node - Correct: Yes\n",
      "\tHit: Restricted Kubernetes dashboard? - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [06:14<01:33,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Giving a docker container a routable ip address\n",
      "\tHit: Assigning IP address to docker containers? - Correct: Yes\n",
      "\tHit: Bind to multiple ip addresses in a single docker container - Correct: Yes\n",
      "\tHit: Run Jupyter Notebook in the Background on Docker - Correct: No\n",
      "\tHit: Start kubernetes container with specific command - Correct: No\n",
      "\tHit: Project layout with vagrant, docker and git - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [06:19<01:29,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes kube-dns pod is pending\n",
      "\tHit: kubelet fails to start with rocket - Correct: Yes\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\tHit: Kubernetes Watch Pod Events with api - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [06:28<01:20,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: kubeadm init fails with incorrect docker version\n",
      "\tHit: kubelet fails to start with rocket - Correct: Yes\n",
      "\tHit: Building WebLogic Docker Image in Ubuntu is not working - Correct: No\n",
      "\tHit: Kubernetes with Vagrant nodes won't register - Correct: Yes\n",
      "\tHit: Kubernetes: spec.clusterIP: invalid value? - Correct: Yes\n",
      "\tHit: Gogits do not start as docker container - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [06:33<01:15,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kafka on Kubernetes multi-node\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: Kubernetes Storage on bare-metal/private cloud - Correct: Yes\n",
      "\tHit: Running dashboard inside play-with-kubernetes - Correct: No\n",
      "\tHit: Kubernetes simple authentication - Correct: Yes\n",
      "\tHit: Kubernetes CGE logging to kibana logging - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [06:56<00:51,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes: change backoffLimit default value\n",
      "\tHit: Configure starting index of StatefulSet's pods in Kubernetes - Correct: Yes\n",
      "\tHit: Kubernetes Ephemeral Storage Limit and Container Logs - Correct: Yes\n",
      "\tHit: Kubernetes: spec.clusterIP: invalid value? - Correct: Yes\n",
      "\tHit: Alternative to Kubernetes rolling update in rest api - Correct: Yes\n",
      "\tHit: Kubernetes fsGroup not changing file ownership on PersistentVolume - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [07:01<00:46,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Kubernetes kube-proxy iptables rules seem to be redundant\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\tHit: Kubernetes kube-dns pod is pending - Correct: Yes\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Access nodeport via kube-proxy from another machine - Correct: Yes\n",
      "\tHit: kubelet fails to start with rocket - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [07:06<00:42,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Allow Spring to have multiple WebMvcConfigurer implementations in different jars\n",
      "\tHit: Spring 5 and hibernate 4 compatible? - Correct: Yes\n",
      "\tHit: Kafka on Kubernetes multi-node - Correct: No\n",
      "\tHit: STOMP over WebSockets: Spring Boot expects JSON; NodeJs STOMP.js client fails to connect - Correct: No\n",
      "\tHit: Thymeleaf: Inserting content of webjar CSS file into style tag - Correct: Yes\n",
      "\tHit: Spring security JWT refresh token not expiring - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [07:11<00:37,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: kubectl : unable to recognize \"csr.yaml\": no matches for kind \"CertificateSigningRequest\" in version \"certificates.k8s.io/v1\"\n",
      "\tHit: Kubectl describe returns 404 for Ingresses - Correct: Yes\n",
      "\tHit: Kubernetes: spec.clusterIP: invalid value? - Correct: Yes\n",
      "\tHit: kubelet fails to start with rocket - Correct: Yes\n",
      "\tHit: Unknown and noisy fields when executing kubectl run and outputting to yaml - Correct: Yes\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [07:34<00:14,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: kubernetes get values from already deployed pod/daemonset\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: Kubernetes kube-dns pod is pending - Correct: Yes\n",
      "\tHit: Running dashboard inside play-with-kubernetes - Correct: No\n",
      "\tHit: Kubernetes Watch Pod Events with api - Correct: Yes\n",
      "\tHit: Kubernetes - Best way to initialize persistent volume for a database deployment - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [07:39<00:09,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Get current resource usage of a pod in Kubernetes with Go client\n",
      "\tHit: Kubernetes Watch Pod Events with api - Correct: Yes\n",
      "\tHit: kubernetes get values from already deployed pod/daemonset - Correct: Yes\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: Can I get a Pod's pause container ID via the Kubernetes API? - Correct: Yes\n",
      "\tHit: Dynamically resolve a pods address for get request - Correct: Yes\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [07:43<00:04,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: kubelet fails to start with rocket\n",
      "\tHit: kubeadm init fails with incorrect docker version - Correct: Yes\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\tHit: Kubernetes kube-dns pod is pending - Correct: Yes\n",
      "\tHit: Kubernetes with Vagrant nodes won't register - Correct: Yes\n",
      "\tHit: Airflow scheduler can not connect to Kubernetes service api - Correct: No\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:48<00:00,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where is the latest protobuf API for the Kubernetes CRI?\n",
      "\tHit: Kubernetes prometheus metrics for running pods and nodes? - Correct: Yes\n",
      "\tHit: Kubernetes Watch Pod Events with api - Correct: Yes\n",
      "\tHit: kubernetes is not routing traffic to grpc service - Correct: No\n",
      "\tHit: What is the syntax of annotation values in Kubernetes? - Correct: Yes\n",
      "\tHit: Restricted Kubernetes dashboard? - Correct: No\n",
      "\n",
      "\n",
      "Total Correct Answers: 185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# print the question where more than 2 hits are correct :\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, ndcg_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"No positive class found in y_true, recall is set to one for all thresholds.\")\n",
    "\n",
    "def compute_cosine_similarity(query_embedding, corpus_embeddings):\n",
    "    # Normalize embeddings\n",
    "    if len(query_embedding.shape) == 1:\n",
    "        query_embedding = query_embedding / torch.norm(query_embedding, keepdim=True)\n",
    "        query_embedding = query_embedding.unsqueeze(0)  # Add an additional dimension\n",
    "    else:\n",
    "        query_embedding = query_embedding / torch.norm(query_embedding, dim=1, keepdim=True)\n",
    "    \n",
    "    corpus_embeddings = corpus_embeddings / torch.norm(corpus_embeddings, dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_similarities = torch.mm(query_embedding, corpus_embeddings.transpose(0, 1))\n",
    "    \n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "\n",
    "def forward_pass_rerank(query, precomputed_embedding=None, val_embeddings=None, top_k=50):\n",
    "    \n",
    "    if precomputed_embedding is None:\n",
    "        q_emb = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    else:\n",
    "        q_emb = precomputed_embedding\n",
    "\n",
    "    # Ensure the query embedding is 2-dimensional\n",
    "    if len(q_emb.shape) == 1:\n",
    "        q_emb = q_emb.unsqueeze(0)\n",
    "\n",
    "    if val_embeddings is None:\n",
    "        raise ValueError(\"No embeddings provided for the validation set.\")\n",
    "    \n",
    "    \n",
    "    if val_embeddings.shape[1] != q_emb.shape[1]:\n",
    "        val_embeddings = val_embeddings.transpose(0, 1)\n",
    "    \n",
    "    cosine_similarities = compute_cosine_similarity(q_emb, val_embeddings)\n",
    "    \n",
    "\n",
    "    top_indices = torch.topk(cosine_similarities, k=top_k+1, dim=1).indices[0].tolist()\n",
    "    hits = [{'corpus_id': index, 'score': cosine_similarities[0, index].item()} for index in top_indices]\n",
    "   \n",
    "\n",
    "    cross_inputs = []\n",
    "    to_remove = -1\n",
    "    for hit in hits:\n",
    "        readme = val_readme[hit['corpus_id']]\n",
    "        text = val_text[hit['corpus_id']]\n",
    "        if query == text:\n",
    "            to_remove = hits.index(hit)\n",
    "        cross_inputs.append([query, readme])\n",
    "    cross_scores = cr_encoder.predict(cross_inputs)\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross_score'] = cross_scores[idx]\n",
    "    if to_remove != -1: del hits[to_remove]\n",
    "    hits = hits[:top_k]\n",
    "\n",
    "    return hits\n",
    "\n",
    "\n",
    "\n",
    "total_correct = 0\n",
    "\n",
    "for (query_key, answers), (_, readme) in tqdm(zip(val_query_answer.items(), val_query_readme.items()), total=len(val_query_answer)):\n",
    "    query= val_corpus[query_key]\n",
    "    # Fetch precomputed embedding\n",
    "    query_id = title_to_id[query]\n",
    "    precomputed_query_embedding = torch.tensor(embeddings_dict.get(str(query_id)))\n",
    "    hits = forward_pass_rerank(query, precomputed_embedding=precomputed_query_embedding, val_embeddings=val_emb, top_k=top_k)\n",
    "\n",
    "    r = val_query_readme[query_key]\n",
    "    \n",
    "    hit_info_list = []\n",
    "\n",
    "    for hit in hits[:5]:\n",
    "        readme = val_readme[hit['corpus_id']]\n",
    "        h_text = val_text[hit['corpus_id']]\n",
    "        cscore_output_cr = cr_encoder.predict([query, readme])  \n",
    "        cscore_cr = cscore_output_cr[0] if isinstance(cscore_output_cr, (list, tuple, np.ndarray)) else cscore_output_cr\n",
    "        hit_info = {\n",
    "            'h_text': h_text,\n",
    "            'cscore': cscore_cr,\n",
    "            'is_correct': \"Yes\" if readme in r else \"No\"\n",
    "        }\n",
    "        hit_info_list.append(hit_info)\n",
    "    \n",
    "    correct_count = sum(1 for hit_info in hit_info_list if hit_info['is_correct'] == \"Yes\")\n",
    "    \n",
    "    # Check if more than 2 hits are correct, and if so, print the question and its hits\n",
    "    if correct_count >= 2:\n",
    "        print(f\"Question: {query}\")\n",
    "        for hit_info in hit_info_list:\n",
    "            print(f\"\\tHit: {hit_info['h_text']} - Correct: {hit_info['is_correct']}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    total_correct += correct_count\n",
    "\n",
    "print(f\"Total Correct Answers: {total_correct}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
