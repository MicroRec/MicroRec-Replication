{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3527,"status":"ok","timestamp":1699501546666,"user":{"displayName":"Ahmed","userId":"18180742192777473891"},"user_tz":-660},"id":"rQdeQ0_s5CJp","outputId":"cf598e72-77d7-44bd-da84-f283175adb03"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer, CrossEncoder, util, models\n","from sklearn.feature_extraction import _stop_words as stop_words\n","from tqdm.notebook import tqdm\n","from rank_bm25 import BM25Okapi\n","\n","import torch\n","import string\n","import json\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","\n","import numpy as np\n","import pandas as pd\n","from collections import Counter"]},{"cell_type":"markdown","metadata":{"id":"CbiMpTzBvWQA"},"source":["# the next cells related to each methods. run one of them only"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"elapsed":27309,"status":"ok","timestamp":1699499803387,"user":{"displayName":"Ahmed","userId":"18180742192777473891"},"user_tz":-660},"id":"rNDKzodUv8U0","outputId":"c4f7fd72-f4bb-4270-d26e-e4aa9a099d08"},"outputs":[],"source":["# for QR512\n","bi_enc_weights = '/content/drive/MyDrive/microservice/bienc-exp7/'\n","cr_enc_weights = '/content/drive/MyDrive/microservice/crenc-readme-exp2/'\n","data_folder = 'generated5'\n","top_k = 50\n","use_base = False\n","\n","import pandas as pd\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import re\n","# Download necessary NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Define stopwords and lemmatizer\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def advanced_clean(text):\n","\n","    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n","\n","    # Tokenization\n","    tokens = word_tokenize(text)\n","\n","    # Lowercasing\n","    tokens = [token.lower() for token in tokens]\n","\n","    # Remove stopwords\n","    tokens = [token for token in tokens if token not in stop_words]\n","\n","    # Lemmatization\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","    # Reconstruct the text\n","    return ' '.join(tokens)\n","file_path = '/content/drive/MyDrive/microservice/20231004_data.xlsx'\n","df = pd.read_excel(file_path, index_col=0)\n","\n","df['readme_short'] = df['readme_short'].astype(str)\n","\n","\n","df['answer'] = df['readme_short'].apply(lambda x: advanced_clean(x))\n","\n","\n","\n","df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"executionInfo":{"elapsed":79358,"status":"ok","timestamp":1699500646731,"user":{"displayName":"Ahmed","userId":"18180742192777473891"},"user_tz":-660},"id":"01ZZwblsvxxJ","outputId":"88e28a73-60c6-4c19-aab5-09e152e8865d"},"outputs":[],"source":["# for QD\n","\n","bi_enc_weights = '/content/drive/MyDrive/microservice/bienc-exp7/'\n","cr_enc_weights = '/content/drive/MyDrive/microservice/crenc-docker-exp2/'\n","data_folder = 'generated5'\n","top_k = 50\n","use_base = False\n","\n","import pandas as pd\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import re\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def advanced_clean(text):\n","\n","    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n","\n","\n","    tokens = word_tokenize(text)\n","\n","\n","    tokens = [token.lower() for token in tokens]\n","\n","\n","    tokens = [token for token in tokens if token not in stop_words]\n","\n","\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","\n","    return ' '.join(tokens)\n","file_path = '/content/drive/MyDrive/microservice/20231004_data.xlsx'\n","df = pd.read_excel(file_path, index_col=0)\n","\n","df['docker'] = df['docker'].astype(str)\n","\n","\n","df['answer'] = df['docker'].apply(lambda x: advanced_clean(x))\n","\n","\n","\n","df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"executionInfo":{"elapsed":34999,"status":"ok","timestamp":1699501589504,"user":{"displayName":"Ahmed","userId":"18180742192777473891"},"user_tz":-660},"id":"dT9VfniR_tLi","outputId":"315cf338-9441-496b-c1de-54f22b78b2d7"},"outputs":[],"source":["# for QRD\n","bi_enc_weights = '/content/drive/MyDrive/microservice/bienc-exp7/'\n","cr_enc_weights = '/content/drive/MyDrive/microservice/crenc-readme-exp4/'\n","data_folder = 'generated5'\n","top_k = 50\n","use_base = False\n","\n","import pandas as pd\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import re\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def advanced_clean(text):\n","\n","    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n","\n","\n","    tokens = word_tokenize(text)\n","\n","\n","    tokens = [token.lower() for token in tokens]\n","\n","\n","    tokens = [token for token in tokens if token not in stop_words]\n","\n","\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","\n","    return ' '.join(tokens)\n","file_path = '/content/drive/MyDrive/microservice/20231004_data.xlsx'\n","df = pd.read_excel(file_path, index_col=0)\n","\n","df['readme_short'] = df['readme_short'].astype(str)\n","df['docker'] = df['docker'].astype(str)\n","\n","df['answer'] = df.apply(lambda row: advanced_clean(' '.join(row['readme_short'].split()[:300] + row['docker'].split()[:200])), axis=1)\n","\n","\n","\n","df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1699501594526,"user":{"displayName":"Ahmed","userId":"18180742192777473891"},"user_tz":-660},"id":"-a6_g7nFL_vk","outputId":"080a8fd7-e485-465e-e2fc-c15ef59d5614"},"outputs":[],"source":["print(df['answer'].iloc[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZO2H41aANV_"},"outputs":[],"source":["english_stopwords = set(stopwords.words('english'))\n","\n","def bm25_tokenizer(text):\n","  tokenized_doc = []\n","  for token in text.lower().split():\n","    token = token.strip(string.punctuation)\n","\n","    if len(token) > 0 and token not in english_stopwords:\n","      tokenized_doc.append(token)\n","\n","  return tokenized_doc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NG181d1lAROy"},"outputs":[],"source":["if use_base:\n","    word_embedding_model = models.Transformer('distilroberta-base', max_seq_length=350)\n","    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n","    bi_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","    cr_encoder = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-6')\n","else:\n","\n","    bi_encoder = SentenceTransformer(bi_enc_weights)\n","\n","    cr_encoder = CrossEncoder(cr_enc_weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9DFayD5HAVuY"},"outputs":[],"source":["with open(f'/content/drive/MyDrive/microservice/test_passage.json', 'r') as f:\n","    val_passage = json.load(f)\n","\n","with open(f'/content/drive/MyDrive/microservice/test_corpus.json', 'r') as f:\n","    val_corpus = json.load(f)\n","\n","val_query_answer = {}\n","val_query_readme = {}\n","for idx, rel in val_passage.items():\n","    pos = rel[0]\n","\n","\n","    readme = df.loc[int(pos[0]), 'answer']\n","    questions = []\n","    for p in pos:\n","        question = df.loc[int(p), 'Question Title']\n","        questions.append(question)\n","    val_query_answer[idx] = questions\n","    val_query_readme[idx] = [readme]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y7c_bg8GAe6X"},"outputs":[],"source":["def shorten(text):\n","    tmp = text.split()[:512]\n","    return ' '.join(tmp)\n","\n","val_text = list(val_corpus.values())\n","\n","val_readme = []\n","for t in val_text:\n","    val_readme.append(df.loc[df['Question Title'] == t, 'answer'].values[0])\n","\n","\n","with open(\"/content/drive/MyDrive/microservice/embeddings_GPT.json\", \"r\") as jsonfile:\n","    embeddings_dict = json.load(jsonfile)\n","\n","val_emb_tensors = []\n","\n","\n","title_to_id = dict(zip(df['Question Title'], df.index))\n","\n","# Convert the embeddings into tensors\n","for text in val_text:\n","\n","    text_id = title_to_id[text]\n","    embedding = embeddings_dict.get(str(text_id))\n","    if embedding:\n","        val_emb_tensors.append(torch.tensor(embedding))\n","\n","\n","val_emb = torch.stack(val_emb_tensors)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1699501623362,"user":{"displayName":"Ahmed","userId":"18180742192777473891"},"user_tz":-660},"id":"dFTumNXnAjVv","outputId":"114f7e80-bbe5-4311-e433-04f973d4d2fd"},"outputs":[],"source":["from tqdm import tqdm\n","tokenized_corpus = []\n","for idx, passage in tqdm(val_corpus.items()):\n","    tokenized_corpus.append(bm25_tokenizer(passage))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFes02-jAuyt"},"outputs":[],"source":["bm25 = BM25Okapi(tokenized_corpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":650610,"status":"ok","timestamp":1699502279391,"user":{"displayName":"Ahmed","userId":"18180742192777473891"},"user_tz":-660},"id":"sH6HOX-qAyl2","outputId":"bc69874c-05ad-49ce-967e-24ddb3fadf15"},"outputs":[],"source":["def evaluate_bm25(query, answer, top_k=50):\n","    # Convert answer to set for faster lookup\n","    answer_set = set(answer)\n","\n","    # Get bm25 scores for the query\n","    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n","\n","    # Check if we have enough scores\n","    if len(bm25_scores) < top_k:\n","        raise ValueError(\"Not enough BM25 scores for top_k.\")\n","\n","    # Retrieve the top-k indices based on bm25_scores\n","    top_n = np.argpartition(bm25_scores, -top_k)[-top_k:]\n","    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n","    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n","\n","    bm25_counter = -1\n","    bm25_map = 0\n","    tmp_hits = 0\n","\n","    # Iterate over top-k hits\n","    for idx, hit in enumerate(bm25_hits):\n","        candidate = val_text[hit['corpus_id']]\n","        if candidate in answer_set:\n","            if bm25_counter == -1:\n","                bm25_counter = idx + 1\n","\n","            tmp_hits += 1\n","            bm25_map += tmp_hits / (idx + 1)\n","            answer_set.remove(candidate)\n","\n","    bm25_map /= len(answer)\n","    bm25_mrr = 1 / bm25_counter if bm25_counter != -1 else 0.0\n","\n","    return bm25_map, bm25_mrr\n","\n","def compute_cosine_similarity(query_embedding, corpus_embeddings):\n","    # Normalize embeddings\n","    if len(query_embedding.shape) == 1:\n","        query_embedding = query_embedding / torch.norm(query_embedding, keepdim=True)\n","        query_embedding = query_embedding.unsqueeze(0)  # Add an additional dimension\n","    else:\n","        query_embedding = query_embedding / torch.norm(query_embedding, dim=1, keepdim=True)\n","\n","    corpus_embeddings = corpus_embeddings / torch.norm(corpus_embeddings, dim=1, keepdim=True)\n","\n","    # Compute cosine similarity\n","    cosine_similarities = torch.mm(query_embedding, corpus_embeddings.transpose(0, 1))\n","\n","    return cosine_similarities\n","\n","\n","def forward_pass_rerank(query, precomputed_embedding=None, val_embeddings=None, top_k=50):\n","\n","    if precomputed_embedding is None:\n","        q_emb = bi_encoder.encode(query, convert_to_tensor=True)\n","    else:\n","        q_emb = precomputed_embedding\n","\n","    # Ensure the query embedding is 2-dimensional\n","    if len(q_emb.shape) == 1:\n","        q_emb = q_emb.unsqueeze(0)\n","\n","    if val_embeddings is None:\n","        raise ValueError(\"No embeddings provided for the validation set.\")\n","\n","\n","    if val_embeddings.shape[1] != q_emb.shape[1]:\n","        val_embeddings = val_embeddings.transpose(0, 1)\n","\n","    cosine_similarities = compute_cosine_similarity(q_emb, val_embeddings)\n","\n","\n","    top_indices = torch.topk(cosine_similarities, k=top_k+1, dim=1).indices[0].tolist()\n","    hits = [{'corpus_id': index, 'score': cosine_similarities[0, index].item()} for index in top_indices]\n","\n","\n","    cross_inputs = []\n","    to_remove = -1\n","    for hit in hits:\n","        readme = val_readme[hit['corpus_id']]\n","        text = val_text[hit['corpus_id']]\n","        if query == text:\n","            to_remove = hits.index(hit)\n","        cross_inputs.append([query, readme])\n","    cross_scores = cr_encoder.predict(cross_inputs)\n","    for idx in range(len(cross_scores)):\n","        hits[idx]['cross_score'] = cross_scores[idx]\n","    if to_remove != -1: del hits[to_remove]\n","    hits = hits[:top_k]\n","\n","    return hits\n","\n","\n","def evaluate_bi_encoder(hits, answer, top_k=50):\n","    answer_set = set(answer)\n","    bi_enc_counter = -1\n","    bi_enc_map = 0\n","    tmp_hits = 0\n","    bi_enc_hit_list = [0] * top_k\n","    bi_enc_hit_recall_list = [0] * top_k\n","    hits = sorted(hits, key=lambda x: x['score'], reverse=True)\n","\n","    for idx, hit in enumerate(hits):\n","        candidate = val_text[hit['corpus_id']]\n","        if candidate in answer_set:\n","            if bi_enc_counter == -1:\n","                bi_enc_counter = idx + 1\n","            bi_enc_hit_list[idx] = 1\n","            bi_enc_hit_recall_list[idx] = 1\n","            tmp_hits += 1\n","            bi_enc_map += tmp_hits / (idx + 1)\n","            answer_set.remove(candidate)\n","\n","    bi_enc_map /= len(answer)\n","    bi_enc_mrr = 1 / bi_enc_counter if bi_enc_counter != -1 else 0.0\n","\n","    return bi_enc_map, bi_enc_mrr, bi_enc_hit_list, bi_enc_hit_recall_list\n","\n","def evaluate_cr_encoder(hits, answer, top_k=50, mode='cross_score'):\n","    answer_set = set(answer)\n","    cr_enc_counter = -1\n","    cr_enc_map = 0\n","    tmp_hits = 0\n","    cr_enc_hit_list = [0] * top_k\n","    cr_enc_hit_recall_list = [0] * top_k\n","    hits = sorted(hits, key=lambda x: x[mode], reverse=True)\n","\n","    for idx, hit in enumerate(hits):\n","        candidate = val_readme[hit['corpus_id']]\n","        if candidate in answer_set:\n","            if cr_enc_counter == -1:\n","                cr_enc_counter = idx + 1\n","            cr_enc_hit_list[idx] = 1\n","            cr_enc_hit_recall_list[idx] = 1\n","            tmp_hits += 1\n","            cr_enc_map += tmp_hits / (idx + 1)\n","            answer_set.remove(candidate)\n","\n","    cr_enc_map /= len(answer)\n","    cr_enc_mrr = 1 / cr_enc_counter if cr_enc_counter != -1 else 0.0\n","\n","    return cr_enc_map, cr_enc_mrr, cr_enc_hit_list, cr_enc_hit_recall_list\n","\n","bm25_scores = {'mrr': 0, 'map': 0}\n","bi_enc_scores = {'mrr': 0, 'map': 0, 'precision': [0] * 4, 'recall': [0] * 4}\n","cr_enc_scores = {'mrr': 0, 'map': 0, 'precision': [0] * 4, 'recall': [0] * 4}\n","\n","for (query_key, answers), (_, readme) in tqdm(zip(val_query_answer.items(), val_query_readme.items()), total=len(val_query_answer)):\n","    query= val_corpus[query_key]\n","    # Fetch precomputed embedding\n","    query_id = title_to_id[query]\n","    precomputed_query_embedding = torch.tensor(embeddings_dict.get(str(query_id)))\n","    hits = forward_pass_rerank(query, precomputed_embedding=precomputed_query_embedding, val_embeddings=val_emb, top_k=top_k)\n","\n","    r = val_query_readme[query_key]\n","    #hits = forward_pass_rerank(query, top_k)\n","\n","\n","    bm25_map, bm25_mrr = evaluate_bm25(query, answers, top_k)\n","    b_map, b_mrr, b_hit, b_rec = evaluate_bi_encoder(hits, answers)\n","    c_map, c_mrr, c_hit, c_rec = evaluate_cr_encoder(hits, r)\n","\n","    tmp_precision = [0] * 4\n","    tmp_recall = [0] * 4\n","    for idx, n in enumerate([1, 3, 5, 10]):\n","        tmp_precision[idx] = sum(b_hit[:n]) / n\n","        tmp_recall[idx] = sum(b_rec[:n]) / len(answers)\n","\n","    bi_enc_scores['precision'] = [x + y for (x, y) in zip(bi_enc_scores['precision'], tmp_precision)]\n","    bi_enc_scores['recall'] = [x + y for (x, y) in zip(bi_enc_scores['recall'], tmp_recall)]\n","\n","    tmp_precision = [0] * 4\n","    tmp_recall = [0] * 4\n","    for idx, n in enumerate([1, 3, 5, 10]):\n","        tmp_precision[idx] = sum(c_hit[:n]) / n\n","        tmp_recall[idx] = sum(c_rec[:n]) / len(answers)\n","\n","    cr_enc_scores['precision'] = [x + y for (x, y) in zip(cr_enc_scores['precision'], tmp_precision)]\n","    cr_enc_scores['recall'] = [x + y for (x, y) in zip(cr_enc_scores['recall'], tmp_recall)]\n","\n","    bm25_scores['map'] += bm25_map\n","    bm25_scores['mrr'] += bm25_mrr\n","\n","    bi_enc_scores['map'] += b_map\n","    bi_enc_scores['mrr'] += b_mrr\n","\n","    cr_enc_scores['map'] += c_map\n","    cr_enc_scores['mrr'] += c_mrr\n","\n","bm25_scores['map'] /= len(val_query_answer)\n","bm25_scores['mrr'] /= len(val_query_answer)\n","bi_enc_scores['map'] /= len(val_query_answer)\n","bi_enc_scores['mrr'] /= len(val_query_answer)\n","bi_enc_scores['precision'] = [x/len(val_query_answer) for x in bi_enc_scores['precision']]\n","bi_enc_scores['recall'] = [x/len(val_query_answer) for x in bi_enc_scores['recall']]\n","cr_enc_scores['map'] /= len(val_query_answer)\n","cr_enc_scores['mrr'] /= len(val_query_answer)\n","cr_enc_scores['precision'] = [x/len(val_query_answer) for x in cr_enc_scores['precision']]\n","cr_enc_scores['recall'] = [x/len(val_query_answer) for x in cr_enc_scores['recall']]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1699502297299,"user":{"displayName":"Ahmed","userId":"18180742192777473891"},"user_tz":-660},"id":"vBN5ekJ7A2ua","outputId":"4e8277a5-991a-41f7-f6b3-6ccd104679a3"},"outputs":[],"source":["print(f'BM25:\\n{json.dumps(bm25_scores, indent=2)}')\n","print(f'Bi-Encoder:\\n{json.dumps(bi_enc_scores, indent=2)}')\n","print(f'Cross-Encoder:\\n{json.dumps(cr_enc_scores, indent=2)}')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMZhWvzPQ7b8vaI8FizS5fo","gpuType":"T4","mount_file_id":"1zErteLpHURbdcWMuwebMaV-8udpAmcpH","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
