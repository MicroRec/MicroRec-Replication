{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util, models, evaluation, losses, InputExample\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import IterableDataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16\n",
    "model_name = 'cross-encoder/ms-marco-TinyBERT-L-6'\n",
    "model_save_path = 'models/crenc-readme-exp2'\n",
    "data_folder = 'generated4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./data/20231004_data.xlsx', index_col=0)\n",
    "\n",
    "\n",
    "df['answer'] = df['readme_short'].astype(str)\n",
    "\n",
    "\n",
    "df = df.dropna()\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplets(Passage_dict):\n",
    "    triplets = []\n",
    "    for k, v in Passage_dict.items():\n",
    "        for x in v[0]:\n",
    "            for y in v[1]:\n",
    "                triplets.append([k, x, y])\n",
    "\n",
    "    return triplets\n",
    "\n",
    "def get_dataset(triplets, corpus):\n",
    "    dataset = []        \n",
    "    for triplet in triplets:\n",
    "        qid, pos_id, neg_id = triplet\n",
    "        \n",
    "        qid = str(qid)\n",
    "        pos_id = pos_id\n",
    "        neg_id = neg_id\n",
    "\n",
    "        query_text = corpus[qid]\n",
    "        pos_text = df.loc[pos_id, 'answer'] \n",
    "        neg_text = df.loc[neg_id, 'answer']\n",
    "\n",
    "        pos_instance = InputExample(texts=[query_text, pos_text],label=1)\n",
    "        neg_instance = InputExample(texts=[query_text, neg_text],label=0)\n",
    "\n",
    "        dataset.append(pos_instance)\n",
    "        dataset.append(neg_instance)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "with open(f'./data/{data_folder}/train_passage.json', 'r') as f:\n",
    "    train_passage = json.load(f)\n",
    "\n",
    "with open(f'./data/{data_folder}/train_corpus.json', 'r') as f:\n",
    "    train_corpus = json.load(f)\n",
    "\n",
    "with open(f'./data/{data_folder}/val_passage.json', 'r') as f:\n",
    "    val_passage = json.load(f)\n",
    "\n",
    "with open(f'./data/{data_folder}/val_corpus.json', 'r') as f:\n",
    "    val_corpus = json.load(f)\n",
    "\n",
    "train_triplets = get_triplets(train_passage)\n",
    "train_dataset = get_dataset(train_triplets, train_corpus)\n",
    "\n",
    "val_triplets = get_triplets(val_passage)\n",
    "val_dataset = get_dataset(val_triplets, val_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='- %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO,\n",
    "    handlers=[LoggingHandler()]\n",
    ")\n",
    "\n",
    "model = CrossEncoder(model_name, max_length=None)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "evaluator = CEBinaryClassificationEvaluator.from_input_examples(val_dataset, name='cross_encoder_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = int(len(train_dataloader) * 2 * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    epochs=2,\n",
    "    evaluation_steps=int(len(train_dataloader) / 2),\n",
    "    warmup_steps=warmup_steps,\n",
    "    save_best_model=True,\n",
    "    output_path=model_save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
